# Carbon-Aware LoRA Hyperparameter Sweep

This directory contains a complete setup for running carbon-aware LoRA fine-tuning experiments on Qwen2.5-Coder-7B-Instruct, demonstrating how intelligent scheduling can reduce carbon emissions by ~40% while maintaining research velocity.

**Related Blog Post**: [Making ML Training Carbon-Aware: Part 2 - Hyperparameter Search with Ray and Compute Gardener](https://www.compute-gardener.com/resources/blog/ml-training-carbon-aware-pt2)

## Overview

Rather than running all experiments immediately or naively delaying everything to the lowest carbon window, we use a **three-tier carbon-aware scheduling strategy** that balances scientific progress with environmental impact.

### The Three-Tier Strategy

**Tier 1: Baseline Variations** (225 gCO2/kWh threshold, 24h max delay)
- Fast feedback on known-good configurations (r=16 experiments)
- Runs in most daily windows including moderate afternoon hours
- 8 experiments exploring learning rates, alpha values, and dropout around proven baseline

**Tier 2: Promising Directions** (175 gCO2/kWh threshold, 48h max delay)
- Balance speed and sustainability for r=32 experiments with standard hyperparameters
- Waits for cleaner solar valley conditions
- 4 experiments stepping up model capacity

**Tier 3: Experimental Long Shots** (125 gCO2/kWh threshold, 96h max delay)
- Maximum carbon optimization for speculative experiments
- r=32 with aggressive/unconventional hyperparameters (alpha=128, dropout=0.15, lr=1e-5)
- 9 experiments that can afford to wait for the cleanest grid windows

### Results from October 28-30, 2025

Running 21 experiments on 2 GPUs (RTX 3090 + RTX A5000):
- **Carbon reduction**: 40.2% (1.81 kgCO2 vs 3.03 kgCO2 immediate execution)
- **Average intensity**: 142 gCO2/kWh vs 226 gCO2/kWh
- **Completion**: All 21/21 jobs completed successfully
- **Wall-clock time**: 51.3 hours (vs ~16 hours immediate execution)
- **Best model**: Tier 3 experiment (r=32, alpha=128, lr=1e-4, dropout=0.05) with 12% better validation loss than baseline

## Components

### 1. Docker Image (`Dockerfile`)
- Base: `rayproject/ray-ml:2.30.0-py310-gpu`
- Upgraded PyTorch 2.2.0 with CUDA 11.8 support
- Upgraded Transformers, PEFT, and Accelerate for modern LLM training

### 2. Training Script (`train_lora.py`)
- Fine-tunes Qwen2.5-Coder-7B-Instruct using LoRA
- CLI arguments for hyperparameter variation (r, alpha, lr, dropout)
- Saves models to persistent storage with timestamps
- Configured for GPUs with 24GB VRAM (e.g., RTX 3090/4090, A5000, A6000)

### 3. Sweep Generator (`generate_sweep.py`)
- Generates RayJob manifests with intelligent tier assignment
- **Core subset mode** (21 experiments): Structured OFAT (one-factor-at-a-time) exploration from baseline
- **Full sweep mode**: Grid search with optional sampling
- Automatically assigns carbon thresholds based on experimental characteristics

### 4. Persistent Storage (`model-storage-pvc.yaml`)
- 20Gi PVC backed by Longhorn
- Stores fine-tuned models at `/mnt/models/`
- Models are timestamped with hyperparameter configuration

### 5. RayJob Manifests
- Generated by `generate_sweep.py` with carbon-aware annotations
- Compute Gardener scheduler integration
- Auto-cleanup after completion

## Quick Start

### Step 1: Create the PVC
```bash
kubectl apply -f model-storage-pvc.yaml
```

### Step 2: Build and push the Docker image
```bash
docker buildx build \
  --platform linux/amd64 \
  --tag dmasselink/ray-ml-modern:2.30.0 --push .
```

### Step 3: Create the training script ConfigMap
```bash
kubectl create configmap llm-finetuning-script \
  --from-file=train_lora.py \
  --namespace=ray-jobs \
  --dry-run=client -o yaml | kubectl apply -f -
```

### Step 4: Generate the hyperparameter sweep

**Core subset (21 experiments, recommended):**
```bash
python generate_sweep.py --output-dir sweep_manifests --core-subset
```

**Full grid search with sampling:**
```bash
python generate_sweep.py \
  --output-dir sweep_manifests \
  --r-values 16 32 \
  --alpha-values 32 64 128 \
  --lr-values 1e-4 5e-5 1e-5 \
  --dropout-values 0.05 0.1 0.15 \
  --sample-size 20
```

### Step 5: Submit experiments

```bash
# Submit all at once
kubectl apply -f sweep_manifests/

# Or submit tier-by-tier for controlled rollout
kubectl apply -f sweep_manifests/*-tier1.yaml  # Baselines first
kubectl apply -f sweep_manifests/*-tier2.yaml  # Then promising directions
kubectl apply -f sweep_manifests/*-tier3.yaml  # Finally long shots
```

### Step 6: Monitor progress

```bash
# Watch all jobs
kubectl get rayjobs -n ray-jobs -w

# Check specific tier
kubectl get rayjobs -n ray-jobs | grep tier1

# View job details
kubectl describe rayjob <job-name> -n ray-jobs
```

## Accessing Fine-tuned Models

Models are saved to the PVC at `/mnt/models/` with timestamps. To access them:

### Option 1: Mount PVC in a pod
```bash
kubectl run -it --rm model-browser \
  --image=ubuntu:22.04 \
  --overrides='
{
  "spec": {
    "containers": [{
      "name": "model-browser",
      "image": "ubuntu:22.04",
      "command": ["/bin/bash"],
      "stdin": true,
      "tty": true,
      "volumeMounts": [{
        "name": "models",
        "mountPath": "/mnt/models"
      }]
    }],
    "volumes": [{
      "name": "models",
      "persistentVolumeClaim": {
        "claimName": "llm-finetuned-models"
      }
    }]
  }
}' \
  --namespace=ray-jobs
```

### Option 2: Copy model out of PVC
Create a pod that copies the model to a local directory or S3.

## Hyperparameter Space

The sweep explores the following ranges:

- **Rank (r)**: [16, 32] - LoRA adapter size (r=64 causes OOM on 24GB VRAM)
- **Alpha (α)**: [32, 64, 128] - Scaling factor for LoRA weights
- **Learning rate**: [1e-4, 5e-5, 1e-5] - Weight update aggressiveness
- **Dropout**: [0.05, 0.1, 0.15] - Regularization strength

### Core Subset: Structured OFAT Exploration

The `--core-subset` flag generates 21 carefully chosen experiments using one-factor-at-a-time (OFAT) methodology:

**Baseline**: r=16, alpha=32, lr=1e-4, dropout=0.1

From this baseline, we systematically vary one parameter at a time, plus a few promising combinations:
- Tier 1 (8 experiments): r=16 variations
- Tier 2 (4 experiments): r=32 with standard hyperparameters
- Tier 3 (9 experiments): r=32 with aggressive/unconventional settings

This provides interpretable results while exploring the hyperparameter space efficiently.

### Training Configuration (Fixed)

- **Model**: Qwen2.5-Coder-7B-Instruct
- **Dataset**: nvidia/HelpSteer2 (5,000 samples: 4,000 train / 1,000 validation)
- **Batch Size**: 2 per GPU, gradient accumulation 4 (effective batch size: 8)
- **Epochs**: 3
- **Scheduler**: Cosine with warmup
- **Target Modules**: All attention and MLP projections

## Deploying Fine-tuned Model with vLLM

vLLM provides an OpenAI-compatible API that works seamlessly with Cline and other tools.

### Step 1: Update the LoRA path in vllm-deployment.yaml
```bash
# Find your trained model timestamp
kubectl exec -it deployment/vllm-lora -n ray-jobs -- ls /mnt/models/

# Edit vllm-deployment.yaml and update the --lora-modules line:
# qwen-lora=/mnt/models/qwen2.5-coder-7b...
```

### Step 2: Deploy vLLM
```bash
kubectl apply -f vllm-deployment.yaml
```

### Step 3: Port-forward to access locally
```bash
kubectl port-forward -n ray-jobs svc/vllm-lora 8000:8000
```

### Step 4: Configure Cline to use your fine-tuned model

In Cline settings, configure a custom OpenAI-compatible endpoint:

```json
{
  "apiProvider": "openai-compatible",
  "openAiCompatible": {
    "baseUrl": "http://localhost:8000/v1",
    "apiKey": "dummy-key",
    "modelId": "Qwen/Qwen2.5-Coder-3B-Instruct:qwen-lora"
  }
}
```

**Note**: The model ID format is `base-model:lora-adapter-name`. The `qwen-lora` part matches what you specified in `--lora-modules`.

### Testing the API

```bash
# Test basic completion
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-Coder-3B-Instruct:qwen-lora",
    "prompt": "def fibonacci(n):",
    "max_tokens": 100
  }'

# Test chat completion (for Cline)
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-Coder-3B-Instruct:qwen-lora",
    "messages": [
      {"role": "user", "content": "Write a Python function to compute fibonacci numbers"}
    ]
  }'
```

## vLLM vs Ollama

**vLLM Advantages:**
- Native LoRA support (no merging needed)
- OpenAI-compatible API out of the box
- Better GPU utilization
- Can serve multiple LoRA adapters simultaneously

**Ollama Advantages:**
- Simpler setup for non-technical users
- Better for CPU-only inference
- Easier model management

For your use case with LoRA fine-tuning on Kubernetes, **vLLM is the better choice**.

## Automatic Cluster Cleanup

The RayJob is configured to automatically clean up the cluster after job completion:

```yaml
shutdownAfterJobFinishes: true
enableInTreeAutoscaling: true
ttlSecondsAfterFinished: 14400  # 4 hours
```

**This means:**
- ✅ Worker and head pods shut down when training completes
- ✅ Compute Gardener properly tracks pod lifecycle
- ✅ Submitter pod remains for log inspection (until TTL expires)
- ✅ No manual cleanup needed

**Verify cleanup:**
```bash
# Watch the cluster shutdown after job completes
kubectl get pods -n ray-jobs -w

# Check RayJob status
kubectl get rayjob -n ray-jobs
```

## Understanding Tier Assignment Logic

The `generate_sweep.py` script automatically assigns carbon tiers based on experimental characteristics:

```python
def assign_tier(config: Dict) -> Tuple[int, str]:
    r = config['r']
    alpha = config['alpha']
    lr = config['lr']
    dropout = config['dropout']

    # Tier 1: Baseline variations (r=16, known good configs)
    if r == 16:
        return (ExperimentTier.BASELINE, "baseline-variation: r=16 proven effective, needs fast feedback")

    # Tier 2 vs 3: Both use r=32, differentiate by hyperparameters
    elif r == 32:
        # Tier 3: Experimental/aggressive settings
        if alpha == 128 or dropout >= 0.15 or lr <= 1e-5:
            return (ExperimentTier.LONGSHOT, "longshot: r=32 with aggressive/unconventional settings")
        # Tier 2: Standard promising directions
        else:
            return (ExperimentTier.PROMISING, "promising: r=32 with standard hyperparameters")
```

Each generated manifest includes:
- Unique job name with hyperparameter values
- Carbon scheduling annotations appropriate for the tier
- Metadata tracking tier rationale and configuration
- Complete RayJob spec with PVC mounts, GPU resources, etc.

## Results Analysis

### Carbon Intensity Tracking

Compute Gardener annotates each pod with initial and bind-time carbon intensity:

```bash
# Check pod annotations for carbon metrics
kubectl get pod <pod-name> -n ray-jobs -o jsonpath='{.metadata.annotations}' | jq

# Example output shows carbon awareness in action:
{
  "compute-gardener-scheduler.kubernetes.io/initial-carbon-intensity": "287.5",
  "compute-gardener-scheduler.kubernetes.io/bind-time-carbon-intensity": "142.3",
  "compute-gardener-scheduler.kubernetes.io/carbon-intensity-threshold": "175.0"
}
```

### Model Quality Assessment

Each training run saves metrics to the PVC. Use both quantitative and qualitative assessment:

**Quantitative**: Validation loss (lower is better, measures generalization)
**Qualitative**: Deploy top candidates with vLLM and test on real coding tasks

From our Oct 28-30 experiment, the top 5 configurations:

| Rank | r | alpha | lr | dropout | Val Loss | Δ vs Baseline | Carbon (gCO2) | Tier |
|------|---|-------|----|---------|---------:|--------------:|--------------:|------|
| 1 | 32 | 128 | 1e-4 | 0.05 | 0.6970 | **-0.0929** | 51.4 | 3 |
| 2 | 32 | 128 | 1e-4 | 0.1 | 0.7010 | **-0.0889** | 94.5 | 3 |
| 3 | 32 | 128 | 1e-4 | 0.15 | 0.7037 | **-0.0862** | 56.1 | 3 |
| 4 | 16 | 128 | 1e-4 | 0.1 | 0.7059 | **-0.0840** | 90.8 | 1 |
| 5 | 16 | 64 | 1e-4 | 0.1 | 0.7490 | **-0.0409** | 70.3 | 1 |

**Baseline** (r=16, alpha=32, lr=1e-4, dropout=0.1): 0.7899 validation loss

**Key findings**:
- Top 3 models all used alpha=128 with r=32 (Tier 3 long shots)
- Best model ran at just 123 gCO2/kWh
- Higher alpha values had more impact than rank increases

## Why This Approach Works

**Distribution prevents stampede**: Using three thresholds (225/175/125 gCO2/kWh) spreads the 21-job workload across multiple low-carbon periods rather than clustering everything at the absolute minimum, which would create GPU queue congestion.

**Priority queue with carbon constraints**: When GPUs become available, the scheduler picks jobs that (1) meet their carbon threshold AND (2) have highest priority (Tier 1 > Tier 2 > Tier 3). This means:
- Critical baseline experiments get faster feedback
- Promising directions run during "good enough" windows
- Expensive long shots wait patiently for near-optimal conditions
- Limited artificial congestion from all jobs targeting the same threshold

**Separation of concerns**:
- Ray: Manages experiment orchestration
- Kubernetes: Allocates GPU resources and pod lifecycle
- Compute Gardener: Gates scheduling based on carbon intensity
- Result: Each layer handles what it's best at

**Trade-off is manageable**: For batch workloads like hyperparameter sweeps, stretching wall-clock time from 16 hours to 51 hours (3.2×) is often acceptable when you get 40% carbon reduction in return.

## Troubleshooting

### Jobs Not Scheduling
Check current carbon intensity and pod events:
```bash
# View scheduler logs
kubectl logs -n kube-system deployment/compute-gardener-scheduler | grep carbon

# Check pod events
kubectl describe pod <pod-name> -n ray-jobs | grep -A 10 Events
```

### OOM Errors
The training script is already optimized for 24GB VRAM:
- Batch size: 2 per GPU
- Gradient accumulation: 4 steps
- Max sequence length: 384 tokens
- r=32 is the practical limit for 7B models at bfloat16

### Model Re-downloading
Models cache to `/mnt/models/.cache` on the PVC. First run downloads, subsequent runs reuse cached models.

## Files in This Directory

- `train_lora.py` - Training script with CLI arguments for hyperparameters
- `generate_sweep.py` - Generates RayJob manifests with tier assignment
- `rayjob-lora.yaml` - Original single-experiment template (for reference)
- `model-storage-pvc.yaml` - PVC manifest for model storage and caching
- `vllm-deployment.yaml` - Deployment manifest for serving fine-tuned models
- `Dockerfile` - Ray ML container with PyTorch/Transformers

## Learn More

For the full story including methodology, results analysis, and carbon reduction strategies, see the [blog post](https://www.compute-gardener.com/resources/blog/ml-training-carbon-aware-pt2).

For questions or to implement carbon-aware ML training in your organization, reach out via the [Compute Gardener website](https://www.compute-gardener.com).
