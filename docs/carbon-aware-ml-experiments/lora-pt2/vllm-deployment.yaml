apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-lora
  namespace: ray-jobs
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-lora
  template:
    metadata:
      labels:
        app: vllm-lora
    spec:
      runtimeClassName: nvidia
      containers:
        - name: vllm
          image: vllm/vllm-openai:latest
          ports:
            - containerPort: 8000
              name: http
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
          command:
            - python3
            - -m
            - vllm.entrypoints.openai.api_server
            - --model
            - Qwen/Qwen2.5-Coder-7B-Instruct
            - --enable-lora
            - --max-lora-rank
            - "32"
            - --lora-modules
            - qwen-lora=/mnt/models/qwen2.5-coder-7b-r32-a128-lr1e-04-d10_20251028_130855 # Update with preferred model
            - --host
            - "0.0.0.0"
            - --port
            - "8000"
            - --gpu-memory-utilization
            - "0.9"
          resources:
            limits:
              nvidia.com/gpu: "1"
              cpu: "8"
              memory: "32Gi"
            requests:
              nvidia.com/gpu: "1"
              cpu: "4"
              memory: "24Gi"
          volumeMounts:
            - name: model-storage
              mountPath: /mnt/models
              readOnly: true
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 30
      volumes:
        - name: model-storage
          persistentVolumeClaim:
            claimName: llm-finetuned-models-rwx
      nodeSelector:
        nvidia.com/gpu.present: "true"
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-lora
  namespace: ray-jobs
spec:
  selector:
    app: vllm-lora
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
      name: http
  type: ClusterIP
